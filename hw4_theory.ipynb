{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.2 Вероятностный смысл регуляризаторов\n",
    "Рассмотрим совместное распределение $x_i$ и $y_i$ $p(x,y|w)$ и семейство априорных распределений для $w$ $\\{q_{\\alpha}(w)|\\alpha\\in A\\}$. Используя метод максимального правдоподобия, получим:\n",
    "$$\\sum\\limits_{i=0}^n\\ln{p(x_i, y_i|w)q_{\\alpha}(w)}\\to\\max\\limits_w$$\n",
    "Если $L(f(x_i),y_i)\\sim-\\ln{p(x_i, y_i|w)}$, а регуляризационное слагаемое $\\sim-\\ln{q_{\\alpha}(w)}$, то рассматриваемая оптиматизационная задача максимизации эквивалента:\n",
    "$$\\sum\\limits_{i=0}^nL(f(x_i), y_i) + \\alpha Reg(w)\\to\\min\\limits_w$$\n",
    "Подставляя разные регуляризаторы, получим:\n",
    "\n",
    "В случае $l1$-регуляризатора $Reg(w)=\\sum\\limits_{j=0}^m|w_j|$ и $w$ имеет распределение Лапласа:\n",
    "$$q_{\\alpha}(w)=\\left(\\frac{\\alpha}{2}\\right)^m\\exp(-\\alpha Reg(w))$$\n",
    "\n",
    "В случае $l2$-регуляризатора $Reg(w)=\\sum\\limits_{j=0}^mw_j^2$ и $w$ имеет нормальное распределение:\n",
    "$$q_{\\sigma^2}(w)=\\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp(-\\frac{Reg(w)}{2\\sigma^2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.3 SVM и максимизация разделяющей полосы\n",
    "Рассмотрим вектора, каждый из которых лежит в классе $y_i\\in\\{-1, 1\\}$. Если они разделимы плоскостью $(w, x) + w_0$, то найдутся $w$ и $w_0$, такие, что ошибок не будет. Попытаемся расположить плоскость так, чтобы ширина разделяющей полосы была наибольшей. Предварительно нормируем $w$ и $w_0$ так, чтобы $\\min\\limits_{i \\in \\overline{1, n}}y_i((w, x_i) + w_0) = 1$. Пусть $x_1$ и $x_2$ - ближайшие к поверхности с разных сторон точки. Тогда ширина полосы:\n",
    "$$S = |(x_1 - x_2, \\frac{w}{||w||})| = |\\frac{(w, x_1) - (w, x_2)}{||w||}|.$$\n",
    "Так как $\\min\\limits_{i \\in \\overline{1, n}}y_i((w, x_i) - w_0) = 1$, ширина равна:\n",
    "$$S = \\frac{w_0 + 1 - (w_0 - 1)}{||w||} = \\frac{2}{||w||}.$$\n",
    "Тогда задачу о максимизации ширины полосы можно сформулировать следующим образом:\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "||w||^2 \\to \\min\\limits_{w, w_0}\\\\\n",
    "y_i((w, x_i) + w_0) \\ge 1, i \\in \\overline{1, n}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "В реальности множества не обязательно разделимы, поэтому нужно ослабить ограничения и вместо запрета на пересечения границы ввести штраф за неправильную классификацию. Для этого позволим $y_i((w, x_i) + w_0)$ быть меньше чем $1$ на некоторую $\\xi_i$, на которые и введем штраф. Тогда задача примет вид:\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "||w||^2 + \\lambda\\sum\\limits_{i=0}^n\\xi_i\\to \\min\\limits_{w, w_0, \\xi}\\\\\n",
    "y_i((w, x_i) + w_0) \\ge 1 - \\xi_i, i \\in \\overline{1, n}\\\\\n",
    "\\xi_i \\ge 0, i \\in \\overline{1, n}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "Посмотрим на условия: $\\xi_i \\ge 1 - y_i((w, x_i) + w_0)$ и $\\xi_i \\ge 0$. Ясно, что минимальная $\\xi_i$, удовлетворяющая им, это $\\max\\{0, 1 - y_i((w, x_i) + w_0)\\}$. Используя это, избавимся от условия в задаче минимизации, записав её в следующем виде:\n",
    "$$||w||^2 + \\lambda\\sum\\limits_{i=0}^n\\max\\{0, 1 - y_i((w, x_i) + w_0)\\}\\to\\min\\limits_{w, w_0}$$\n",
    "Что и требовалось доказать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.4 Kernel trick\n",
    "Рассмотрим квадратичное ядро:\n",
    "$$K(a, b) = (a_1b_1 + a_2b_2)^2=a_1^2b_1^2 + 2a_1a_2b_1b_2 + a_2^2b_2^2=\n",
    "(a_1^2, \\sqrt{2}a_1a_2)\\begin{pmatrix}b_1^2\\\\\\sqrt{2}b_1b_2\\\\b_2^2\\end{pmatrix}.$$\n",
    "Заметим, что применение этого ядра эквивалентно переходу из исходного пространства в некоторое другое по формуле:\n",
    "$$(a_1, a_2) \\to (a_1^2, \\sqrt{2}a_1a_2, a_2^2)$$\n",
    "Плоскость в новом пространстве будет иметь вид:\n",
    "$$(a_1^2, \\sqrt{2}a_1a_2)\\begin{pmatrix}w_1\\\\w_2\\\\w_3\\end{pmatrix} + w_0 = w_1a_1^2 + w_2\\sqrt{2}a_1a_2 + w_3a_2^2 + w_0 = 0.$$\n",
    "Тогда при $w = (1, 0, 2)$ и $w_0 = -3$ поподание на эту плоскость в новом пространстве эквивалентно попаданию на поверхность $x_1^2 + 2x_2^2 = 3$ в старом. Таким образом, это ядро позволит линейному классификатору построить такую разделяющую поверхность, причем размерность спрямляющего пространства равна трем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.5 l1-регуляризация\n",
    "Рассмотрим задачу:\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "\\sum\\limits_{i=1}^nL(x_i, w, y_i)\\to\\min\\limits_w\\\\\n",
    "\\sum\\limits_{j=1}^m|w_j| \\le \\alpha\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "Учитывая\n",
    "$$\\sum\\limits_{i=1}^nL(x_i, w, y_i) + \\lambda(\\sum\\limits_{j=1}^m|w_j| - \\alpha) =$$\n",
    "$$=\\sum\\limits_{i=1}^nL(x_i, w, y_i) + \\lambda\\sum\\limits_{j=1}^m|w_j| + const,$$\n",
    "по теореме Куна-Такера эта задача эквивалентна задаче:\n",
    "$$\\sum\\limits_{i=1}^nL(x_i, w, y_i) + \\lambda\\sum\\limits_{j=1}^m|w_j| \\to\\min\\limits_w$$\n",
    "$$\\lambda \\ge 0$$\n",
    "$$\\lambda = 0 \\vee \\sum\\limits_{j=1}^m|w_j| = \\alpha$$\n",
    "для некоторого $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.6 Повторение: метрики качества\n",
    "Это уже было в 3.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
